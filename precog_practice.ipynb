{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYwXm1wLXx81Qc7HW3O4eX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHOTU-A/Ampili/blob/master/precog_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaOjjD2FJHV1",
        "outputId": "90efb2c3-bb37-4546-89c4-a957133d5606"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence_transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence_transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4jFGbyh-iLw",
        "outputId": "c557ea40-affb-43c2-8dee-e9db5c68637c"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fw1GBE1p7ACS",
        "outputId": "5b95b9bb-92b4-4cdc-b9b5-29bb6ab2dd94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wcd5JTYPGZyZ",
        "outputId": "0b6c3bd4-b4b5-4e9b-d0dc-246f0070d7fb"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cpu)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.11.12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Dense Representations**\n",
        "“you shall know a word by the company it keeps” - J.R Firth\n",
        "\n",
        "The goal of Part 1 is to explore methods for generating dense word embeddings from a corpus and assess their quality through various evaluations. These embeddings capture semantic meaning of words in a continuous vector space, allowing for improved NLP applications like similarity measurements, clustering, and analogy tasks.\n"
      ],
      "metadata": {
        "id": "CxJNFXhLa7nR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are determining the sentiment of the text from multiple languages."
      ],
      "metadata": {
        "id": "OU3iCx6Ztn97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Load a pre-trained multilingual model\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Create a sentiment analysis pipeline\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Function to analyse sentiment of texts in different languages\n",
        "def analyze_sentiment(texts):\n",
        "    for text in texts:\n",
        "        result = sentiment_analyzer(text)\n",
        "        print(f\"Text: {text}\\nSentiment: {result[0]['label']}, Score: {result[0]['score']:.4f}\\n\")\n",
        "\n",
        "# Example multilingual texts\n",
        "texts = [\n",
        "    \"I love natural language processing!\",  # English\n",
        "    \"J'adore le traitement automatique du langage.\",  # French\n",
        "    \"Me encanta el procesamiento del lenguaje natural.\",  # Spanish\n",
        "    \"Ich liebe die Verarbeitung natürlicher Sprache.\",  # German\n",
        "    \"私は自然言語処理が大好きです！\"  # Japanese\n",
        "]\n",
        "\n",
        "# Call the function with example texts\n",
        "analyze_sentiment(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-3qZ8N47ESO",
        "outputId": "9395557d-6a2c-42e2-948d-119eb1aefea5"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: I love natural language processing!\n",
            "Sentiment: 5 stars, Score: 0.7727\n",
            "\n",
            "Text: J'adore le traitement automatique du langage.\n",
            "Sentiment: 5 stars, Score: 0.6266\n",
            "\n",
            "Text: Me encanta el procesamiento del lenguaje natural.\n",
            "Sentiment: 5 stars, Score: 0.8199\n",
            "\n",
            "Text: Ich liebe die Verarbeitung natürlicher Sprache.\n",
            "Sentiment: 5 stars, Score: 0.7261\n",
            "\n",
            "Text: 私は自然言語処理が大好きです！\n",
            "Sentiment: 5 stars, Score: 0.6448\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#computing the cosine similarity for different sentences\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. Load a pre-aligned multilingual model\n",
        "# This model maps 50+ languages into the same vector space\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# 2. Sentences in different languages\n",
        "sentences_en = [\"The cat is sitting outside.\", \"A man is playing guitar.\"]\n",
        "sentences_es = [\"El gato está sentado afuera.\", \"El clima es agradable.\"]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings_en = model.encode(sentences_en)\n",
        "embeddings_es = model.encode(sentences_es)\n",
        "\n",
        "# 4. Compute cosine similarity between English and Spanish sentences\n",
        "for i, en_vec in enumerate(embeddings_en):\n",
        "    for j, es_vec in enumerate(embeddings_es):\n",
        "        score = util.cos_sim(en_vec, es_vec)\n",
        "        print(f\"EN: {sentences_en[i]} | ES: {sentences_es[j]} | Similarity: {score.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jR_Q0tvGdEp",
        "outputId": "9bf5c324-9bc8-46c2-9f1d-dbd9f7b36f03"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: The cat is sitting outside. | ES: El gato está sentado afuera. | Similarity: 0.9895\n",
            "EN: The cat is sitting outside. | ES: El clima es agradable. | Similarity: 0.1306\n",
            "EN: A man is playing guitar. | ES: El gato está sentado afuera. | Similarity: -0.0006\n",
            "EN: A man is playing guitar. | ES: El clima es agradable. | Similarity: 0.0504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have considered the text from English and Spanish Languages to determine the similarity of the sentences. I will be doing it by considering the 2 objects in each language."
      ],
      "metadata": {
        "id": "FNsA88tsuh8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cosine similarity between English and Spanish sentences\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. Load a pre-aligned multilingual model\n",
        "# This model maps 50+ languages into the same vector space\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "# 2. Sentences in different languages\n",
        "sentences_en = [\"Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information.\", \"Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential.\"]\n",
        "sentences_es = [\"El gato está sentado afuera.\", \"El clima es agradable.\"]\n",
        "                #sentences = [s.strip() for s in sentences if s.strip()]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings_en = model.encode(sentences_en)\n",
        "embeddings_es = model.encode(sentences_es)\n",
        "\n",
        "# 4. Compute cosine similarity between English and Spanish sentences\n",
        "for i, en_vec in enumerate(embeddings_en):\n",
        "    for j, es_vec in enumerate(embeddings_es):\n",
        "        score = util.cos_sim(en_vec, es_vec)\n",
        "        print(f\"EN: {sentences_en[i]} | ES: {sentences_es[j]} | Similarity: {score.item():.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ir727JOHApU",
        "outputId": "17b6daf3-1607-4fc6-98d2-aa222b97d080"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information. | ES: El gato está sentado afuera. | Similarity: -0.0520\n",
            "EN: Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information. | ES: El clima es agradable. | Similarity: 0.0037\n",
            "EN: Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential. | ES: El gato está sentado afuera. | Similarity: -0.0187\n",
            "EN: Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential. | ES: El clima es agradable. | Similarity: -0.0352\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###I need to reduces the spaces witihin the sentneces and store them into the list. Then we need to put it into the list"
      ],
      "metadata": {
        "id": "jDSjYvcUKX7z"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_en = [\"Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information.Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential.\"]"
      ],
      "metadata": {
        "id": "O7NxCRrONPJE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences_hi = [\"शिक्षा व्यक्तिगत और सामाजिक विकास की नींव है।भविष्य की पीढ़ियों के मानस को आकार देने में शिक्षक महत्वपूर्ण भूमिका निभाते हैं।कई विकासशील क्षेत्रों में गुणवत्तापूर्ण स्कूली शिक्षा तक पहुँच एक महत्वपूर्ण चुनौती बनी हुई है।डिजिटल लर्निंग प्लेटफॉर्म ने छात्रों के सूचना प्राप्त करने के तरीके में क्रांति ला दी है ।प्रारंभिक बचपन की शिक्षा दीर्घकालिक संज्ञानात्मक और सामाजिक विकास के लिए महत्वपूर्ण है।विश्वविद्यालय नवाचार, अनुसंधान और आलोचनात्मक सोच के केंद्रों के रूप में कार्य करते हैं।तेजी से बदलती वैश्विक अर्थव्यवस्था में प्रतिस्पर्धी बने रहने के लिए आजीवन सीखना आवश्यक है।एसटीईएम (STEM) विषय छात्रों को आवश्यक समस्या-समाधान और विश्लेषणात्मक कौशल विकसित करने में मदद करते हैं।वित्तीय बाधाएं अक्सर प्रतिभाशाली छात्रों को उच्च शिक्षा प्राप्त करने से रोकती हैं।समावेशी शिक्षा यह सुनिश्चित करती है कि विकलांग बच्चों को सीखने के समान अवसर मिलें।मानकीकृत परीक्षण शिक्षकों और नीति निर्माताओं के बीच एक विवादास्पद विषय बना हुआ है।प्रशिक्षित कर्मचारियों द्वारा सही ढंग से लागू किए जाने पर कक्षा की तकनीक जुड़ाव बढ़ा सकती है।व्यावसायिक प्रशिक्षण विशिष्ट करियर पथों और ट्रेडों के लिए व्यावहारिक कौशल प्रदान करता है।साक्षरता दर किसी राष्ट्र के समग्र मानव विकास का एक प्रमुख संकेतक है।आलोचनात्मक सोच छात्रों को जानकारी का निष्पक्ष और तार्किक रूप से मूल्यांकन करने की अनुमति देती है।वैश्विक महामारी के दौरान करोड़ों छात्रों के लिए दूरस्थ शिक्षा एक आवश्यकता बन गई।संचार और टीम वर्क जैसे सॉफ्ट स्किल्स को आधुनिक नियोक्ताओं द्वारा अत्यधिक महत्व दिया जाता है।स्कूलों के लिए सार्वजनिक वित्तपोषण अक्सर राजनीतिक और आर्थिक बहस का विषय होता है।मेंटरशिप कार्यक्रम छात्रों को उनकी शैक्षणिक और व्यावसायिक यात्राओं को तय करने में मदद करते हैं।शिक्षा का प्राथमिक लक्ष्य व्यक्तियों को उनकी पूर्ण क्षमता तक पहुँचने के लिए सशक्त बनाना है।\"]"
      ],
      "metadata": {
        "id": "xlkr8ghFPW1z"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2: Cross-lingual Alignment**\n",
        "\n",
        "\n",
        "**Problem statement:**\n",
        "In Part 2, you will extend your analysis to two different languages, exploring how embeddings can be aligned across linguistic boundaries. The goal is to design and evaluate methods for cross-lingual alignment, ultimately enabling knowledge transfer between languages.\n",
        "Take any pre-trained monolingual word embeddings for English and Hindi.\n",
        "Align the embeddings of English and Hindi by learning a transformation. One such technique that allows you to learn a transformation is: Procrustes analysis. This is just to give you a direction, you are encouraged to experiment with other alignment techniques.\n",
        "Now you have cross lingual aligned embeddings. How can you evaluate if the cross lingual alignment was effective? Please come up with an effective strategy to quantitatively answer this question.\n"
      ],
      "metadata": {
        "id": "wVpduToVYUNJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Cross-lingual alignment** refers to the process of mapping linguistic units (words, sentences, or entities) from two or more different languages into a single, shared \"embedding space.\" When successful, a word like \"apple\" in English and \"manzana\" in Spanish will have nearly identical vector representations.\n",
        "Here is a guide to building a cross-lingual alignment project in Python, ranging from word-level to sentence-level methods.\n",
        "1. Project Types & Modern Tools\n",
        "Depending on your goals, you will likely fall into one of these three categories:\n",
        "Alignment Level\tGoal\tBest Tool/Library\n",
        "Word Alignment\tMapping words (e.g., dictionary creation)\tMUSE (Facebook), fastText\n",
        "Sentence Alignment\tComparing whole sentences (e.g., search/similarity)\tSentence-Transformers (SBERT), LaBSE\n",
        "Corpus Alignment\tAligning a book/document to its translation\tLingtrain-Aligner, Bleualign\n",
        "\n",
        "\n",
        "2. Core Implementation: Sentence-Level Alignment\n",
        "This is the most common modern use case (e.g., for cross-lingual semantic search). We use **Sentence-Transformers (SBERT)** because it provides pre-aligned multilingual models out of the box."
      ],
      "metadata": {
        "id": "9KiZpzhIQ5rN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# 1. Load a pre-aligned multilingual model\n",
        "# This model maps 50+ languages into the same vector space\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "sentences_en = [\"Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information.Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential.\"]\n",
        "sentences_hi = [\"शिक्षा व्यक्तिगत और सामाजिक विकास की नींव है।भविष्य की पीढ़ियों के मानस को आकार देने में शिक्षक महत्वपूर्ण भूमिका निभाते हैं।कई विकासशील क्षेत्रों में गुणवत्तापूर्ण स्कूली शिक्षा तक पहुँच एक महत्वपूर्ण चुनौती बनी हुई है।डिजिटल लर्निंग प्लेटफॉर्म ने छात्रों के सूचना प्राप्त करने के तरीके में क्रांति ला दी है ।प्रारंभिक बचपन की शिक्षा दीर्घकालिक संज्ञानात्मक और सामाजिक विकास के लिए महत्वपूर्ण है।विश्वविद्यालय नवाचार, अनुसंधान और आलोचनात्मक सोच के केंद्रों के रूप में कार्य करते हैं।तेजी से बदलती वैश्विक अर्थव्यवस्था में प्रतिस्पर्धी बने रहने के लिए आजीवन सीखना आवश्यक है।एसटीईएम (STEM) विषय छात्रों को आवश्यक समस्या-समाधान और विश्लेषणात्मक कौशल विकसित करने में मदद करते हैं।वित्तीय बाधाएं अक्सर प्रतिभाशाली छात्रों को उच्च शिक्षा प्राप्त करने से रोकती हैं।समावेशी शिक्षा यह सुनिश्चित करती है कि विकलांग बच्चों को सीखने के समान अवसर मिलें।मानकीकृत परीक्षण शिक्षकों और नीति निर्माताओं के बीच एक विवादास्पद विषय बना हुआ है।प्रशिक्षित कर्मचारियों द्वारा सही ढंग से लागू किए जाने पर कक्षा की तकनीक जुड़ाव बढ़ा सकती है।व्यावसायिक प्रशिक्षण विशिष्ट करियर पथों और ट्रेडों के लिए व्यावहारिक कौशल प्रदान करता है।साक्षरता दर किसी राष्ट्र के समग्र मानव विकास का एक प्रमुख संकेतक है।आलोचनात्मक सोच छात्रों को जानकारी का निष्पक्ष और तार्किक रूप से मूल्यांकन करने की अनुमति देती है।वैश्विक महामारी के दौरान करोड़ों छात्रों के लिए दूरस्थ शिक्षा एक आवश्यकता बन गई।संचार और टीम वर्क जैसे सॉफ्ट स्किल्स को आधुनिक नियोक्ताओं द्वारा अत्यधिक महत्व दिया जाता है।स्कूलों के लिए सार्वजनिक वित्तपोषण अक्सर राजनीतिक और आर्थिक बहस का विषय होता है।मेंटरशिप कार्यक्रम छात्रों को उनकी शैक्षणिक और व्यावसायिक यात्राओं को तय करने में मदद करते हैं।शिक्षा का प्राथमिक लक्ष्य व्यक्तियों को उनकी पूर्ण क्षमता तक पहुँचने के लिए सशक्त बनाना है।\"]\n",
        "# 4. Compute cosine similarity between English and Spanish sentences\n",
        "# 3. Compute embeddings\n",
        "embeddings_en = model.encode(sentences_en)\n",
        "embeddings_hi = model.encode(sentences_hi)\n",
        "\n",
        "#We are computing the cosine similarity of each sentence of different laguage in the list.\n",
        "for i, en_vec in enumerate(embeddings_en):\n",
        "    for j, hi_vec in enumerate(embeddings_hi):\n",
        "        score = util.cos_sim(en_vec, hi_vec)\n",
        "        print(f\"EN: {sentences_en[i]} | ES: {sentences_hi[j]} | Similarity: {score.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCCp3DLdPs50",
        "outputId": "175308f0-6607-451a-8107-4bb556936b7f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EN: Education is the foundation of personal and societal growth.Teachers play a vital role in shaping the minds of future generations.Access to quality schooling remains a significant challenge in many developing regions.Digital learning platforms have revolutionized the way students access information.Early childhood education is critical for long-term cognitive and social development.Universities serve as centers for innovation, research, and critical thinking.Lifelong learning is essential to remain competitive in a rapidly changing global economy.STEM subjects help students develop essential problem-solving and analytical skills.Financial barriers often prevent talented students from pursuing higher education.Inclusive education ensures that children with disabilities have equal opportunities to learn.Standardized testing remains a controversial topic among educators and policymakers.Classroom technology can enhance engagement when implemented correctly by trained staff.Vocational training provides practical skills for specific career paths and trades.Literacy rates are a key indicator of a nation's overall human development.Critical thinking allows students to evaluate information objectively and logically.Remote learning became a necessity for millions of students during the global pandemic.Soft skills like communication and teamwork are highly valued by modern employers.Public funding for schools is a frequent subject of political and economic debate.Mentorship programs help students navigate their academic and professional journeys.The primary goal of education is to empower individuals to reach their full potential. | ES: शिक्षा व्यक्तिगत और सामाजिक विकास की नींव है।भविष्य की पीढ़ियों के मानस को आकार देने में शिक्षक महत्वपूर्ण भूमिका निभाते हैं।कई विकासशील क्षेत्रों में गुणवत्तापूर्ण स्कूली शिक्षा तक पहुँच एक महत्वपूर्ण चुनौती बनी हुई है।डिजिटल लर्निंग प्लेटफॉर्म ने छात्रों के सूचना प्राप्त करने के तरीके में क्रांति ला दी है ।प्रारंभिक बचपन की शिक्षा दीर्घकालिक संज्ञानात्मक और सामाजिक विकास के लिए महत्वपूर्ण है।विश्वविद्यालय नवाचार, अनुसंधान और आलोचनात्मक सोच के केंद्रों के रूप में कार्य करते हैं।तेजी से बदलती वैश्विक अर्थव्यवस्था में प्रतिस्पर्धी बने रहने के लिए आजीवन सीखना आवश्यक है।एसटीईएम (STEM) विषय छात्रों को आवश्यक समस्या-समाधान और विश्लेषणात्मक कौशल विकसित करने में मदद करते हैं।वित्तीय बाधाएं अक्सर प्रतिभाशाली छात्रों को उच्च शिक्षा प्राप्त करने से रोकती हैं।समावेशी शिक्षा यह सुनिश्चित करती है कि विकलांग बच्चों को सीखने के समान अवसर मिलें।मानकीकृत परीक्षण शिक्षकों और नीति निर्माताओं के बीच एक विवादास्पद विषय बना हुआ है।प्रशिक्षित कर्मचारियों द्वारा सही ढंग से लागू किए जाने पर कक्षा की तकनीक जुड़ाव बढ़ा सकती है।व्यावसायिक प्रशिक्षण विशिष्ट करियर पथों और ट्रेडों के लिए व्यावहारिक कौशल प्रदान करता है।साक्षरता दर किसी राष्ट्र के समग्र मानव विकास का एक प्रमुख संकेतक है।आलोचनात्मक सोच छात्रों को जानकारी का निष्पक्ष और तार्किक रूप से मूल्यांकन करने की अनुमति देती है।वैश्विक महामारी के दौरान करोड़ों छात्रों के लिए दूरस्थ शिक्षा एक आवश्यकता बन गई।संचार और टीम वर्क जैसे सॉफ्ट स्किल्स को आधुनिक नियोक्ताओं द्वारा अत्यधिक महत्व दिया जाता है।स्कूलों के लिए सार्वजनिक वित्तपोषण अक्सर राजनीतिक और आर्थिक बहस का विषय होता है।मेंटरशिप कार्यक्रम छात्रों को उनकी शैक्षणिक और व्यावसायिक यात्राओं को तय करने में मदद करते हैं।शिक्षा का प्राथमिक लक्ष्य व्यक्तियों को उनकी पूर्ण क्षमता तक पहुँचने के लिए सशक्त बनाना है। | Similarity: 0.9133\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "o_rLwOyqQO9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237f56d9-d11c-4ed6-e93c-dbc24204a1b8"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Adjust sep to your delimiter: \",\" or \"\\s+\" for spaces, \"\\t\" for tabs\n",
        "hin_df = pd.read_table(\"/content/sample_data/hin_news_2019_30K-sentences.txt\", header=None)\n",
        "\n",
        "# Optional: set column names\n",
        "hin_df.columns = [\"id\", \"text\"]\n",
        "\n",
        "#df.to_csv(\"hin_news_sentences.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "Wb3LVrbcy5Ho"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hin_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "AXmWuWlu77aH",
        "outputId": "1a1ee558-cdaf-4bd0-cf4b-ed5d0a534d71"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                               text\n",
              "0   1  ‘‘100 लाख करोड़ का निवेश बुनियादी सुविधाओं के ...\n",
              "1   2  १२ डॉक्टरों सहित २२ लोगों ही टीम ने लगातार १४ ...\n",
              "2   3                             १५ प्रतिशत पर आ गई है।\n",
              "3   4  '165 लीची खाने के बाद घटेगी शरीर में ग्लूकोज क...\n",
              "4   5  '1983 जैसी नहीं है PAK टीम, वर्ल्ड कप में टीम ..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ccfbd324-46f2-4197-9fbd-881f934cf1fd\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>‘‘100 लाख करोड़ का निवेश बुनियादी सुविधाओं के ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>१२ डॉक्टरों सहित २२ लोगों ही टीम ने लगातार १४ ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>१५ प्रतिशत पर आ गई है।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>'165 लीची खाने के बाद घटेगी शरीर में ग्लूकोज क...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>'1983 जैसी नहीं है PAK टीम, वर्ल्ड कप में टीम ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ccfbd324-46f2-4197-9fbd-881f934cf1fd')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ccfbd324-46f2-4197-9fbd-881f934cf1fd button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ccfbd324-46f2-4197-9fbd-881f934cf1fd');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1f109716-1a7f-4e29-a2f0-55ff1ef10a87\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f109716-1a7f-4e29-a2f0-55ff1ef10a87')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1f109716-1a7f-4e29-a2f0-55ff1ef10a87 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "hin_df",
              "summary": "{\n  \"name\": \"hin_df\",\n  \"rows\": 27959,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8573,\n        \"min\": 1,\n        \"max\": 30000,\n        \"num_unique_values\": 27959,\n        \"samples\": [\n          12123,\n          12537,\n          8531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27959,\n        \"samples\": [\n          \"\\u091b\\u0939 \\u0926\\u093f\\u0938\\u0902\\u092c\\u0930 1992 \\u0915\\u094b \\u0935\\u093f\\u0935\\u093e\\u0926\\u093f\\u0924 \\u092c\\u093e\\u092c\\u0930\\u0940 \\u092e\\u0938\\u094d\\u091c\\u093f\\u0926 \\u0927\\u094d\\u0935\\u0938\\u094d\\u0924 \\u0939\\u094b\\u0928\\u0947 \\u0915\\u0947 \\u092c\\u093e\\u0926 \\u0926\\u0947\\u0936 \\u092d\\u0930 \\u092e\\u0947\\u0902 \\u0939\\u0941\\u090f \\u0926\\u0902\\u0917\\u094b\\u0902 \\u092e\\u0947\\u0902 2000 \\u0938\\u0947 \\u091c\\u093c\\u094d\\u092f\\u093e\\u0926\\u093e \\u0932\\u094b\\u0917 \\u092e\\u093e\\u0930\\u0947 \\u0917\\u092f\\u0947.\",\n          \"\\u2018\\u091c\\u092f \\u0936\\u094d\\u0930\\u0940 \\u0930\\u093e\\u092e\\u2019 \\u0915\\u0947 \\u0928\\u093e\\u0930\\u0947 \\u0915\\u094b \\u0932\\u0947\\u0915\\u0930 \\u092e\\u092e\\u0924\\u093e \\u092c\\u0928\\u0930\\u094d\\u091c\\u0940 \\u0928\\u0947 \\u090f\\u0915 \\u092c\\u093e\\u0930 \\u092b\\u093f\\u0930 \\u0916\\u094b\\u092f\\u093e \\u0905\\u092a\\u0928.\",\n          \"\\u090f\\u0915\\u094d\\u0936\\u0928 \\u0938\\u0947 \\u092d\\u0930\\u092a\\u0942\\u0930 \\u090b\\u0924\\u093f\\u0915 \\u0930\\u094b\\u0936\\u0928 \\u0914\\u0930 \\u0915\\u091f\\u0930\\u0940\\u0928\\u093e \\u0915\\u0948\\u092b \\u0915\\u0940 \\u092b\\u093f\\u0932\\u094d\\u092e \\u092c\\u0948\\u0902\\u0917 \\u092c\\u0948\\u0902\\u0917 2014 \\u092e\\u0947\\u0902 \\u0930\\u093f\\u0932\\u0940\\u091c \\u0939\\u0941\\u0908 \\u0925\\u0940.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hin_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC1Kfw3TXSgS",
        "outputId": "f50eb0d8-f038-41c3-d586-a6feeccfba41"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['id', 'text'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(hin_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqFU8vBP0MFb",
        "outputId": "eb4a9ce7-d1a5-4267-fb2a-4e160225e6fa"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id                                               text\n",
            "0          1  ‘‘100 लाख करोड़ का निवेश बुनियादी सुविधाओं के ...\n",
            "1          2  १२ डॉक्टरों सहित २२ लोगों ही टीम ने लगातार १४ ...\n",
            "2          3                             १५ प्रतिशत पर आ गई है।\n",
            "3          4  '165 लीची खाने के बाद घटेगी शरीर में ग्लूकोज क...\n",
            "4          5  '1983 जैसी नहीं है PAK टीम, वर्ल्ड कप में टीम ...\n",
            "...      ...                                                ...\n",
            "27954  29996  ह्यूंदै कोना के साथ 2.8 kW पोर्टेबल चार्जर भी ...\n",
            "27955  29997  ह्यूंदै ने तो 9 जुलाई को इलेक्ट्रिक SUV KONA भ...\n",
            "27956  29998  िकस्सा कोताह यह है कि बावजूद इस खुलासे के पायल...\n",
            "27957  29999  िफलहाल अगर पीडि़तों से बात करो, तो अपनों के गम...\n",
            "27958  30000  िफलहाल यह जोड़ा कहाँ है, नहीं मालूम; लेकिन यह ...\n",
            "\n",
            "[27959 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Adjust sep to your delimiter: \",\" or \"\\s+\" for spaces, \"\\t\" for tabs\n",
        "eng_df = pd.read_table(\"/content/sample_data/eng_news_2019_30K-sentences.txt\", header=None)\n",
        "\n",
        "# Optional: set column names\n",
        "eng_df.columns = [\"id\", \"text\"]\n",
        "\n",
        "#df.to_csv(\"hin_news_sentences.csv\", index=False)"
      ],
      "metadata": {
        "id": "V1pZezBj8D0X"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(eng_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXPccphd8QK4",
        "outputId": "ea142cf3-e506-404a-ad82-f1c354c9c856"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          id                                               text\n",
            "0          1  £1.2 billion is a huge amount and that was con...\n",
            "1          2  2019 will be about simplifying and declutterin...\n",
            "2          3  • $20 million for the construction of a climat...\n",
            "3          4  $284 million bond package for Del Valle ISD wi...\n",
            "4          5  '£286,000 or thereabouts was spent on catering...\n",
            "...      ...                                                ...\n",
            "26575  29996  Zverev found some composure to win his next se...\n",
            "26576  29997  Z will screen at 10 p.m. on Sept. 20 at the Gl...\n",
            "26577  29998  ┬а So, even if your version is a last-minute c...\n",
            "26578  29999                        ┬аThis is a hands-on class.\n",
            "26579  30000  РђюJason is the unfortunate one to miss out,Рђ...\n",
            "\n",
            "[26580 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeQaMHuA7b6d",
        "outputId": "a2dc355e-2ef1-4282-8d2f-7d3f57521ef4"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from NLTK) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from NLTK) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from NLTK) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from NLTK) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict, Counter\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jfk3C3Z0P5q",
        "outputId": "fdf67990-93e9-44af-e136-31d4090fa84a"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCaNIRUf7i1l",
        "outputId": "46b121f2-8906-48f2-8288-23e398edf62b"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df1 =  pd.read_table('/content/sample_data/hin_news_2019_30K-sentences.txt')"
      ],
      "metadata": {
        "id": "hmpUzUql-sMd"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_table('/content/sample_data/eng_news_2019_30K-sentences.txt')"
      ],
      "metadata": {
        "id": "ubTYEKPRGlmO"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  30,000 sentences of English, Hindi news are stored in the data frame"
      ],
      "metadata": {
        "id": "Cror7igV-ydo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0kQ8YAd-013",
        "outputId": "88b2f51f-4adc-485d-a1cc-bf332d5171f5"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           1  \\\n",
            "0          2   \n",
            "1          3   \n",
            "2          4   \n",
            "3          5   \n",
            "4          6   \n",
            "...      ...   \n",
            "27953  29996   \n",
            "27954  29997   \n",
            "27955  29998   \n",
            "27956  29999   \n",
            "27957  30000   \n",
            "\n",
            "      ‘‘100 लाख करोड़ का निवेश बुनियादी सुविधाओं के लिए अगले 5 साल में किया जाएगा।’  \n",
            "0      १२ डॉक्टरों सहित २२ लोगों ही टीम ने लगातार १४ ...                             \n",
            "1                                 १५ प्रतिशत पर आ गई है।                             \n",
            "2      '165 लीची खाने के बाद घटेगी शरीर में ग्लूकोज क...                             \n",
            "3      '1983 जैसी नहीं है PAK टीम, वर्ल्ड कप में टीम ...                             \n",
            "4      ’ 2379 किलोग्राम आॅर्बिटर के मिशन का जीवन काल ...                             \n",
            "...                                                  ...                             \n",
            "27953  ह्यूंदै कोना के साथ 2.8 kW पोर्टेबल चार्जर भी ...                             \n",
            "27954  ह्यूंदै ने तो 9 जुलाई को इलेक्ट्रिक SUV KONA भ...                             \n",
            "27955  िकस्सा कोताह यह है कि बावजूद इस खुलासे के पायल...                             \n",
            "27956  िफलहाल अगर पीडि़तों से बात करो, तो अपनों के गम...                             \n",
            "27957  िफलहाल यह जोड़ा कहाँ है, नहीं मालूम; लेकिन यह ...                             \n",
            "\n",
            "[27958 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_f3DBzOTG2mW",
        "outputId": "f151746d-eb5b-411b-8310-66cc3c982825"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           1  \\\n",
            "0          2   \n",
            "1          3   \n",
            "2          4   \n",
            "3          5   \n",
            "4          6   \n",
            "...      ...   \n",
            "26574  29996   \n",
            "26575  29997   \n",
            "26576  29998   \n",
            "26577  29999   \n",
            "26578  30000   \n",
            "\n",
            "      £1.2 billion is a huge amount and that was constraining us in the speed of transformation.'  \n",
            "0      2019 will be about simplifying and declutterin...                                           \n",
            "1      • $20 million for the construction of a climat...                                           \n",
            "2      $284 million bond package for Del Valle ISD wi...                                           \n",
            "3      '£286,000 or thereabouts was spent on catering...                                           \n",
            "4      $2.87 Earnings Per Share Expected for Berkshir...                                           \n",
            "...                                                  ...                                           \n",
            "26574  Zverev found some composure to win his next se...                                           \n",
            "26575  Z will screen at 10 p.m. on Sept. 20 at the Gl...                                           \n",
            "26576  ┬а So, even if your version is a last-minute c...                                           \n",
            "26577                        ┬аThis is a hands-on class.                                           \n",
            "26578  РђюJason is the unfortunate one to miss out,Рђ...                                           \n",
            "\n",
            "[26579 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp1 = pd.DataFrame(df2,columns=['id','text',])"
      ],
      "metadata": {
        "id": "ygb_4YCHMWQg"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seaborn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M72w2ox09ga9",
        "outputId": "9ea00692-1f2e-4789-c62f-a6dd3d6f3b6a"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    # Lowercase and remove punctuation\n",
        "    text = text.lower()\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    return text.split()\n"
      ],
      "metadata": {
        "id": "Ccdoha2q_B2z"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = eng_df['text']"
      ],
      "metadata": {
        "id": "qlzWI5LUQ91X"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = eng_df.id, eng_df.text"
      ],
      "metadata": {
        "id": "3e_C5HNq_toU"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "AHN4EhcnFMEI"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuMc9UNTFn2E",
        "outputId": "e96330ea-b719-4ab7-c398-e46713a610cd"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "from scipy.sparse import csr_matrix\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# 1. Download and Prepare Data\n",
        "nltk.download('brown')\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "\n",
        "\n",
        "# Preprocess: Lowercase, remove non-alphabetic tokens, and (optional) remove stop words\n",
        "# Removing stop words makes the relationships much clearer for small datasets\n",
        "sentences = []\n",
        "for sent in y:\n",
        "    clean_sent = [w.lower() for w in sent if w.isalpha() and w.lower() not in stop_words]\n",
        "    if clean_sent:\n",
        "        sentences.append(clean_sent)\n",
        "\n",
        "# 2. Build Vocabulary (Top 5,000 words for 20k sentences)\n",
        "all_words = [word for sent in sentences for word in sent]\n",
        "vocab_counts = Counter(all_words)\n",
        "VOCAB_SIZE = 5000\n",
        "most_common = vocab_counts.most_common(VOCAB_SIZE)\n",
        "word2id = {word: i for i, (word, _) in enumerate(most_common)}\n",
        "id2word = {i: word for word, i in word2id.items()}\n",
        "\n",
        "# 3. Function to Build Matrix\n",
        "def create_matrix(data, word_map, window_size):\n",
        "    cooc_counts = defaultdict(int)\n",
        "    for sent in data:\n",
        "        for i, word in enumerate(sent):\n",
        "            if word not in word_map: continue\n",
        "\n",
        "            target_id = word_map[word]\n",
        "\n",
        "            # Define window boundaries\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(sent), i + window_size + 1)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if i == j: continue\n",
        "                if sent[j] in word_map:\n",
        "                    context_id = word_map[sent[j]]\n",
        "                    cooc_counts[(target_id, context_id)] += 1\n",
        "\n",
        "    # Convert to Sparse Matrix\n",
        "    rows, cols, values = [], [], []\n",
        "    for (r, c), count in cooc_counts.items():\n",
        "        rows.append(r)\n",
        "        cols.append(c)\n",
        "        values.append(count)\n",
        "\n",
        "    return csr_matrix((values, (rows, cols)), shape=(len(word_map), len(word_map)))\n",
        "\n",
        "# 4. Experiment: Compare Window Sizes\n",
        "test_word = \"word\"\n",
        "window_options = [2, 5, 10]\n",
        "\n",
        "print(f\"Analyzing top neighbors for '{test_word}' across different window sizes:\\n\")\n",
        "\n",
        "for w in window_options:\n",
        "    matrix = create_matrix(sentences, word2id, w)\n",
        "\n",
        "    # Get the row for our test word\n",
        "    if test_word in word2id:\n",
        "        word_idx = word2id[test_word]\n",
        "        row = matrix.getrow(word_idx).toarray()[0]\n",
        "\n",
        "        # Get indices of highest counts\n",
        "        top_indices = np.argsort(row)[::-1][:5]\n",
        "        neighbors = [id2word[i] for i in top_indices]\n",
        "\n",
        "        print(f\"Window Size {w}: {', '.join(neighbors)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1cZzM5kI4oX",
        "outputId": "dbc693e0-d8ea-466b-cf38-5eff59a78483"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing top neighbors for 'word' across different window sizes:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1: Dense Representations**\n",
        "  \n",
        "\n",
        "1. **Construct a co-occurrence matrix**  I have constructed the co-occurence matrix that tracks how frequently pairs of words appear together. Note:- I have faced issue while processing 30,000 sentences.Hence i have considered a sample of 30 sentnence to construct the co-occurence matrix."
      ],
      "metadata": {
        "id": "QmfTqIDA1hyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_cooccurrence_matrix(sentences, word2id, window_size):\n",
        "    # We use a dictionary of counters for (row, col) pairs\n",
        "    # In larger production systems, we'd use (data, row, col) lists directly for COO matrix\n",
        "    cooc_counts = defaultdict(int)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        for i, word in enumerate(sentence):\n",
        "            if word not in word2id: continue\n",
        "\n",
        "            target_id = word2id[word]\n",
        "\n",
        "            # Look at neighbors within window_size\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(sentence), i + window_size + 1)\n",
        "\n",
        "            for j in range(start, end):\n",
        "                if i == j: continue # Skip the word itself\n",
        "                context_word = sentence[j]\n",
        "                if context_word in word2id:\n",
        "                    context_id = word2id[context_word]\n",
        "                    cooc_counts[(target_id, context_id)] += 1"
      ],
      "metadata": {
        "id": "Q6jpR7NQATy-"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Lowercase and keep only alphabetic tokens\n",
        "    tokens = re.findall(r\"[a-zA-Z]+\", text.lower())\n",
        "    return tokens\n",
        "\n",
        "# computation of co-occurence matrix\n",
        "def build_cooccurrence_matrix(sentences, window_size=2, min_count=1):\n",
        "    \"\"\"\n",
        "    sentences: list of raw text sentences (e.g., 30 English sentences)\n",
        "    window_size: symmetric context window size around each word\n",
        "    min_count: discard words whose total frequency < min_count\n",
        "    \"\"\"\n",
        "    # 1) Tokenize each sentence\n",
        "    tokenized = [tokenize(s) for s in sentences]\n",
        "\n",
        "    # 2) Build vocabulary with frequency filtering\n",
        "    freq = Counter(w for sent in tokenized for w in sent)\n",
        "    vocab = [w for w, c in freq.items() if c >= min_count]\n",
        "    vocab.sort()\n",
        "    word2id = {w: i for i, w in enumerate(vocab)}\n",
        "    V = len(vocab)\n",
        "\n",
        "    # 3) Count co-occurrences\n",
        "    co_counts = defaultdict(Counter)\n",
        "    for sent in tokenized:\n",
        "        ids = [word2id[w] for w in sent if w in word2id]\n",
        "        n = len(ids)\n",
        "        for i, center in enumerate(ids):\n",
        "            # context window [i-window, i+window]\n",
        "            left = max(0, i - window_size)\n",
        "            right = min(n, i + window_size + 1)\n",
        "            for j in range(left, right):\n",
        "                if j == i:\n",
        "                    continue\n",
        "                context = ids[j]\n",
        "                co_counts[center][context] += 1\n",
        "\n",
        "    # 4) Convert to dense matrix (V x V)\n",
        "    mat = np.zeros((V, V), dtype=int)\n",
        "    for i, row_counts in co_counts.items():\n",
        "        for j, c in row_counts.items():\n",
        "            mat[i, j] = c\n",
        "\n",
        "    # 5) Wrap in a DataFrame for readability\n",
        "    df = pd.DataFrame(mat, index=vocab, columns=vocab)\n",
        "    return df\n",
        "\n",
        "# Example usage with 30 sentences . We have considered around 30 sentences regarding the topic Research\n",
        "sentences = [\n",
        "    \"Research is a systematic process of inquiry to discover new information.\",\n",
        "\"It involves collecting, documenting, and analyzing data.\",\n",
        "    \"Research aims to build credibility and support ideas with facts.\",\n",
        "\"It helps in narrowing down topics and evaluating information quality.\",\n",
        "\"Research encourages curiosity and awareness of issues.\",\n",
        "\"It can be funded by public, private, or charitable organizations.\",\n",
        "\"Scientific research explains the properties of the world.\",\n",
        "\"It makes practical applications possible in many areas.\",\n",
        "\"Researchers study various disciplines, from biology to history.\",\n",
        "\"A research methodology provides a structured approach to data collection.\"\n",
        "\"This method ensures ethical data handling and transparency.\",\n",
        "\"Strong research builds a foundation for future studies.\",\n",
        "\"Exploratory research provides foundational knowledge for deeper studies.\",\n",
        "\"It's essential for expanding knowledge and understanding.\",\n",
        "\"Research is crucial in healthcare for developing new treatments.\",\n",
        "\"It drives innovation in technology.\",\n",
        "\"Social research raises awareness about societal challenges.\",\n",
        "\"Many projects require years of dedicated research.\",\n",
        "\"A researcher might spend months gathering literature.\",\n",
        "\"Funding for research is vital for progress.\",\n",
        "\"Research seeks to find truth and improve human life.\",\n",
        "\"It helps in creating new concepts and understandings.\"\n",
        "\"Researchers use specific tools and techniques.\",\n",
        "\"Valid research allows others to verify findings.\",\n",
        "\"It can involve field studies on research vessels.\",\n",
        "\"Research can involve extensive testing and data analysis.\",\n",
        "\"It’s about generating new knowledge or applying existing knowledge creatively.\",\n",
        "\"From buying a car to curing diseases, research informs decisions.\",\n",
        "\"The process allows for building on past work.\",\n",
        "\"Ultimately, research advances human understanding.\"\n",
        "    \"This is a sample sentence for the co occurrence matrix example.\",\n",
        "    \"Co occurrence matrices capture how often words appear together in context.\",\n",
        "    \"Word co occurrence is a basic distributional semantics tool.\",\n",
        "\n",
        "]\n",
        "\n",
        "co_matrix_df = build_cooccurrence_matrix(sentences, window_size=4, min_count=1)    #Window size  =2\n",
        "print(co_matrix_df)   #Here N =172 (Vocabulary size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsXGvjvyEkGQ",
        "outputId": "dfb14672-a1e1-43af-a83c-d2ef16fb64f8"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          a  about  advances  aims  allows  analysis  analyzing  and  appear  \\\n",
            "a         2      0         0     0       0         0          0    0       0   \n",
            "about     0      0         0     0       0         0          0    0       0   \n",
            "advances  0      0         0     0       0         0          0    0       0   \n",
            "aims      0      0         0     0       0         0          0    1       0   \n",
            "allows    0      0         0     0       0         0          0    0       0   \n",
            "...      ..    ...       ...   ...     ...       ...        ...  ...     ...   \n",
            "word      1      0         0     0       0         0          0    0       0   \n",
            "words     0      0         0     0       0         0          0    0       1   \n",
            "work      0      0         0     0       0         0          0    0       0   \n",
            "world     0      0         0     0       0         0          0    0       0   \n",
            "years     0      0         0     0       0         0          0    0       0   \n",
            "\n",
            "          applications  ...  various  verify  vessels  vital  with  word  \\\n",
            "a                    0  ...        0       0        0      0     0     1   \n",
            "about                0  ...        0       0        0      0     0     0   \n",
            "advances             0  ...        0       0        0      0     0     0   \n",
            "aims                 0  ...        0       0        0      0     0     0   \n",
            "allows               0  ...        0       1        0      0     0     0   \n",
            "...                ...  ...      ...     ...      ...    ...   ...   ...   \n",
            "word                 0  ...        0       0        0      0     0     0   \n",
            "words                0  ...        0       0        0      0     0     0   \n",
            "work                 0  ...        0       0        0      0     0     0   \n",
            "world                0  ...        0       0        0      0     0     0   \n",
            "years                0  ...        0       0        0      0     0     0   \n",
            "\n",
            "          words  work  world  years  \n",
            "a             0     0      0      0  \n",
            "about         0     0      0      0  \n",
            "advances      0     0      0      0  \n",
            "aims          0     0      0      0  \n",
            "allows        0     0      0      0  \n",
            "...         ...   ...    ...    ...  \n",
            "word          0     0      0      0  \n",
            "words         0     0      0      0  \n",
            "work          0     0      0      0  \n",
            "world         0     0      0      0  \n",
            "years         0     0      0      0  \n",
            "\n",
            "[172 rows x 172 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(co_matrix_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UK5jRLmhU6wg",
        "outputId": "9c0dce8e-3832-40e2-98bf-80f4f3fcd706"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          a  about  advances  aims  allows  analysis  analyzing  and  appear  \\\n",
            "a         2      0         0     0       0         0          0    0       0   \n",
            "about     0      0         0     0       0         0          0    0       0   \n",
            "advances  0      0         0     0       0         0          0    0       0   \n",
            "aims      0      0         0     0       0         0          0    1       0   \n",
            "allows    0      0         0     0       0         0          0    0       0   \n",
            "...      ..    ...       ...   ...     ...       ...        ...  ...     ...   \n",
            "word      1      0         0     0       0         0          0    0       0   \n",
            "words     0      0         0     0       0         0          0    0       1   \n",
            "work      0      0         0     0       0         0          0    0       0   \n",
            "world     0      0         0     0       0         0          0    0       0   \n",
            "years     0      0         0     0       0         0          0    0       0   \n",
            "\n",
            "          applications  ...  various  verify  vessels  vital  with  word  \\\n",
            "a                    0  ...        0       0        0      0     0     1   \n",
            "about                0  ...        0       0        0      0     0     0   \n",
            "advances             0  ...        0       0        0      0     0     0   \n",
            "aims                 0  ...        0       0        0      0     0     0   \n",
            "allows               0  ...        0       1        0      0     0     0   \n",
            "...                ...  ...      ...     ...      ...    ...   ...   ...   \n",
            "word                 0  ...        0       0        0      0     0     0   \n",
            "words                0  ...        0       0        0      0     0     0   \n",
            "work                 0  ...        0       0        0      0     0     0   \n",
            "world                0  ...        0       0        0      0     0     0   \n",
            "years                0  ...        0       0        0      0     0     0   \n",
            "\n",
            "          words  work  world  years  \n",
            "a             0     0      0      0  \n",
            "about         0     0      0      0  \n",
            "advances      0     0      0      0  \n",
            "aims          0     0      0      0  \n",
            "allows        0     0      0      0  \n",
            "...         ...   ...    ...    ...  \n",
            "word          0     0      0      0  \n",
            "words         0     0      0      0  \n",
            "work          0     0      0      0  \n",
            "world         0     0      0      0  \n",
            "years         0     0      0      0  \n",
            "\n",
            "[172 rows x 172 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "def reduce_cooccurrence_svd(co_matrix_df, d=86):\n",
        "    \"\"\"\n",
        "    co_matrix_df : pandas DataFrame of shape (N, N)\n",
        "                   word-word co-occurrence matrix.\n",
        "    d            : target dimension (d << N), e.g., 86 for N=172.\n",
        "\n",
        "    Returns:\n",
        "        embeddings_df : DataFrame of shape (N, d)\n",
        "                        each row is a d-dimensional embedding.\n",
        "    \"\"\"\n",
        "    # N x N matrix\n",
        "    X = co_matrix_df.values.astype(float)\n",
        "\n",
        "    # Optional: log-scaling to dampen very large counts\n",
        "    # X = np.log1p(X)\n",
        "\n",
        "    # Truncated SVD\n",
        "    svd = TruncatedSVD(n_components=d, n_iter=20, random_state=0)\n",
        "    U_d = svd.fit_transform(X)   # shape: (N, d)\n",
        "\n",
        "    # Optional: scale by sqrt of singular values (closer to full SVD embeddings)\n",
        "    # S_d = svd.singular_values_\n",
        "    # U_d = U_d * np.sqrt(S_d)\n",
        "\n",
        "    # Wrap back to DataFrame\n",
        "    embeddings_df = pd.DataFrame(\n",
        "        U_d,\n",
        "        index=co_matrix_df.index,\n",
        "        columns=[f\"dim_{i+1}\" for i in range(d)]\n",
        "    )\n",
        "    return embeddings_df\n",
        "\n",
        "# Example:\n",
        "# co_matrix_df = ...  # 172x172 DataFrame from your co-occurrence step\n",
        "# embeddings_172x86 = reduce_cooccurrence_svd(co_matrix_df, d=86)\n",
        "#print(embeddings_df.shape)  # (172, 86)\n"
      ],
      "metadata": {
        "id": "dn71BO9BUg7i"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def cosine_similarity(v1, v2):\n",
        "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
        "\n",
        "def evaluate_embeddings(matrix, word2idx, simlex_path):\n",
        "    # 1. Standard Benchmark: SimLex-999 Spearman Correlation\n",
        "    simlex = pd.read_csv(simlex_path, sep='\\t')\n",
        "    model_scores = []\n",
        "    human_scores = []\n",
        "\n",
        "    for _, row in simlex.iterrows():\n",
        "        w1, w2 = row['word1'], row['word2']\n",
        "        if w1 in word2idx and w2 in word2idx:\n",
        "            v1 = matrix[word2idx[w1]]\n",
        "            v2 = matrix[word2idx[w2]]\n",
        "            model_scores.append(cosine_similarity(v1, v2))\n",
        "            human_scores.append(row['SimLex999'])\n",
        "\n",
        "    correlation, _ = spearmanr(model_scores, human_scores)\n",
        "    print(f\" SimLex-999 Spearman Correlation: {correlation:.4f}\")\n",
        "\n",
        "    # 2. SURPRISE: \"Semantic Neighborhood Density\" Analysis\n",
        "    # Does the model cluster abstract words more loosely than concrete words?\n",
        "    # SimLex provides 'conc(w1)'. We check if higher concreteness = higher local density.\n",
        "    avg_neighbor_dist = []\n",
        "    concreteness = []\n",
        "\n",
        "    for _, row in simlex.drop_duplicates(subset=['word1']).head(100).iterrows():\n",
        "        w1 = row['word1']\n",
        "        if w1 in word2idx:\n",
        "            v1 = matrix[word2idx[w1]]\n",
        "            # Calculate mean distance to 50 random words (global density)\n",
        "            sampled_indices = np.random.choice(len(matrix), 50)\n",
        "            dists = [cosine_similarity(v1, matrix[i]) for i in sampled_indices]\n",
        "            avg_neighbor_dist.append(np.mean(dists))\n",
        "            concreteness.append(row['conc(w1)'])\n",
        "\n",
        "    conc_corr, _ = spearmanr(avg_neighbor_dist, concreteness)\n",
        "    print(f\" Surprise Metric (Conc vs Density Corr): {conc_corr:.4f}\")\n",
        "\n",
        "    # 3. Visualization: PCA vs t-SNE\n",
        "    plot_words = ['king', 'queen', 'man', 'woman', 'apple', 'fruit', 'car', 'bus']\n",
        "    indices = [word2idx[w] for w in plot_words if w in word2idx]\n",
        "    vectors = matrix[indices]\n",
        "\n",
        "    tsne = TSNE(n_components=2, perplexity=5, random_state=42)\n",
        "    reduced = tsne.fit_transform(vectors)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, word in enumerate(plot_words):\n",
        "        plt.scatter(reduced[i, 0], reduced[i, 1])\n",
        "        plt.annotate(word, (reduced[i, 0], reduced[i, 1]))\n",
        "    plt.title(\"t-SNE Projection of Semantic Clusters\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hD18uMFdVMTo"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Uyoi7YnsbU_q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4937114b-dd72-4a9a-db6f-bd7900d7025e"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VguRbUhspwQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Procrustes analysis**"
      ],
      "metadata": {
        "id": "CD8dMkOIqAbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.linalg import orthogonal_procrustes\n",
        "\n",
        "def align_embeddings(source_matrix, target_matrix):\n",
        "    \"\"\"\n",
        "    Learns the transformation matrix W using Procrustes Analysis.\n",
        "    source_matrix (X): Embeddings of English words in the seed dictionary.\n",
        "    target_matrix (Y): Embeddings of their Hindi translations.\n",
        "    \"\"\"\n",
        "    # R is the transformation matrix, scale is the scale factor\n",
        "    R, scale = orthogonal_procrustes(source_matrix, target_matrix)\n",
        "    return R\n",
        "\n",
        "# --- Simulation ---\n",
        "# Assume we have 5 seed word pairs (e.g., \"king\" -> \"राजा\")\n",
        "# Each embedding is 300 dimensions\n",
        "dim = 300\n",
        "num_seed_words = 500\n",
        "\n",
        "# Mock pre-trained embeddings\n",
        "eng_seed_vectors = np.random.randn(num_seed_words, dim)\n",
        "hin_seed_vectors = np.random.randn(num_seed_words, dim)\n",
        "\n",
        "# 1. Learn the alignment\n",
        "transformation_matrix = align_embeddings(eng_seed_vectors, hin_seed_vectors)\n",
        "\n",
        "# 2. Map an English word to the Hindi space\n",
        "# Let 'test_vec' be an English word vector not in the seed set\n",
        "test_vec = np.random.randn(1, dim)\n",
        "mapped_vec = np.dot(test_vec, transformation_matrix)"
      ],
      "metadata": {
        "id": "PnzmMOK0YoZt"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mapped_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkUVLJXMhg8z",
        "outputId": "a2d78eee-0dfa-41ee-ec5e-647ce239fc8b"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4.38972429e-01 -3.75051533e-01 -4.27471406e-01 -5.72737939e-01\n",
            "   1.03116340e+00  4.05649659e-02 -1.02861277e+00  8.39920305e-01\n",
            "  -3.47860485e-01  4.69752142e-01 -3.25309050e-01 -2.27025818e-02\n",
            "   6.36262791e-01 -3.55605967e-01  8.42841778e-01 -1.23186911e+00\n",
            "   3.04291756e+00  6.89642319e-01  9.53659796e-02  5.36099263e-01\n",
            "  -7.63248261e-01  2.62193849e-02  3.58688765e-02 -4.95440430e-01\n",
            "  -6.81145756e-01 -7.47102247e-02 -6.79887662e-01 -1.26222759e-01\n",
            "  -1.15862135e-01  6.69194756e-01 -9.07250270e-01 -5.80491126e-01\n",
            "  -2.73756958e-01  5.30560570e-01  2.12026390e-01  4.42715856e-01\n",
            "  -4.40073175e-01  5.23701621e-01 -3.29843784e-01 -1.97968481e-01\n",
            "   1.14102620e+00 -9.07032588e-01 -5.80519001e-01  1.85407964e+00\n",
            "  -2.30035499e+00 -4.53964785e-01  1.72600448e+00  1.66011025e-01\n",
            "  -8.28143616e-01  2.59881230e+00 -8.39708483e-01 -6.75618773e-01\n",
            "   1.43254804e-02 -8.04588532e-01  3.74835384e-02  4.40553853e-01\n",
            "   5.07151642e-01  7.43842197e-01  7.05675533e-01 -3.26150068e-01\n",
            "  -3.79329029e-01  9.25537774e-01  3.82569077e-01  1.13021572e+00\n",
            "  -4.51484061e-01 -9.09681538e-01 -1.89342155e+00  4.87777490e-01\n",
            "   1.24958782e+00  1.34461367e+00  3.30686543e-02 -1.62007363e+00\n",
            "  -3.01049867e-01  5.30258962e-01 -1.07752383e+00 -9.56039894e-01\n",
            "  -4.51442335e-01 -6.99909485e-01 -4.44375015e-02 -1.96905081e+00\n",
            "  -9.47985906e-01 -4.27016195e-01 -1.81286151e+00 -8.35775566e-01\n",
            "  -1.54088294e+00 -2.59701665e-01 -8.28665728e-02  8.02203980e-01\n",
            "   4.02616678e-01 -9.98288414e-01  5.23424645e-01  1.76857404e-01\n",
            "  -2.10818928e+00 -8.48417910e-01  2.17538491e+00  1.27860551e+00\n",
            "   1.10073302e+00 -1.99524562e-01  3.26662673e-02 -2.42643474e-01\n",
            "  -6.93056057e-01 -6.00208607e-01  1.87089408e+00 -1.86503397e-01\n",
            "  -1.09934481e+00  1.12574498e+00  7.23303718e-01 -5.65652415e-01\n",
            "  -1.26544011e+00  1.69768878e+00  5.81975367e-01  6.52569287e-01\n",
            "   3.11591978e-01 -9.47851122e-01 -3.58474333e-01 -6.68144464e-02\n",
            "   1.03515361e+00  8.17357424e-02  1.12123190e+00 -2.21991981e+00\n",
            "   6.67474243e-01  2.37613388e+00 -1.58680363e+00  9.07336477e-01\n",
            "  -4.93227035e-01  1.07861552e+00  7.36482956e-01 -4.61414489e-01\n",
            "   1.04581111e+00  2.04976912e-01 -4.90732301e-02  2.18425909e-02\n",
            "  -5.62714739e-02 -1.65283839e-02  3.03010182e+00 -7.99040409e-01\n",
            "   5.43626292e-01  4.77420852e-01 -1.35850368e+00  1.83784303e-02\n",
            "  -1.37729617e+00  1.22313630e+00 -4.97378388e-01 -1.03855768e+00\n",
            "   6.18478973e-01  7.36674198e-01  8.34685458e-02  1.25643295e+00\n",
            "  -1.04718007e+00 -9.27215785e-01 -7.42692722e-01  2.91835663e-01\n",
            "  -1.24122212e+00 -1.48303455e+00  8.56664160e-01 -1.60134872e+00\n",
            "  -5.02289165e-01 -6.13529091e-02 -2.12266385e-03 -1.72444946e-01\n",
            "   1.07587791e+00  1.11408059e-01  2.43379365e+00 -4.11207240e-01\n",
            "   1.08234498e+00  1.42875245e+00  7.65300328e-01  1.54001549e+00\n",
            "  -2.20519383e-01 -2.65945448e-01 -1.77029231e-01  6.92724285e-01\n",
            "  -1.61914191e+00 -9.41831005e-01 -6.56723797e-01 -2.05361291e-01\n",
            "   1.99984189e+00 -6.42082857e-01  6.45812926e-01  4.59774807e-01\n",
            "  -4.02156770e-01 -6.84517289e-01  1.63025219e+00  2.01340401e-01\n",
            "   1.38499616e+00  6.24709425e-01 -3.81256489e-01  8.00572780e-01\n",
            "  -1.53702232e+00  2.83655910e-02  8.05644480e-01  1.25760031e-01\n",
            "   6.87249151e-01  6.00663389e-01 -8.60992149e-02  1.12994165e+00\n",
            "   7.17714900e-01  1.34564506e+00  1.14713216e-01  2.41100612e-01\n",
            "   9.66255148e-01 -2.17136294e-02 -1.57008835e+00 -1.00115918e+00\n",
            "  -1.97502257e-01  1.58934564e+00  1.14040294e+00  5.32785575e-01\n",
            "   1.98937955e+00 -3.59434677e+00 -2.19141670e-01 -9.60204614e-01\n",
            "  -5.84807911e-01  1.05230771e+00 -1.44644046e+00 -1.00537926e+00\n",
            "  -2.61288410e-02  1.38134986e+00  2.06706291e-01  6.06818323e-01\n",
            "   1.29213985e+00  1.54450702e+00 -2.40179478e-01 -2.02381685e+00\n",
            "  -3.08863702e-01 -2.89809388e-01  7.98362371e-01  3.22808003e-01\n",
            "  -5.84524228e-01  1.91692804e+00 -3.23360826e-01  1.42928921e-01\n",
            "  -8.41162623e-01  1.25725619e-02 -4.19439560e-02 -5.83346156e-02\n",
            "  -2.97703186e-01  1.84469929e+00  5.71374091e-01 -8.23016416e-02\n",
            "  -8.53007773e-01 -8.25325761e-01  1.15236538e+00  6.35386694e-01\n",
            "   9.94245283e-01  1.06307305e+00 -9.04788376e-01 -1.59102049e+00\n",
            "   4.68066147e-01  4.35728965e-01  6.77170581e-01 -3.28841842e-01\n",
            "   1.07786237e+00  1.24564312e+00 -8.35179282e-01  9.35669724e-01\n",
            "   7.40684273e-01  1.12915473e+00  4.08142696e-01 -3.61761941e-01\n",
            "   1.28576801e+00  2.01036269e-01  4.60770480e-01 -8.62197750e-01\n",
            "   5.43008678e-01  7.89745628e-01  2.71749836e-01 -1.85425308e-01\n",
            "  -5.64938334e-01 -7.17941920e-01 -1.15375690e+00  6.37927599e-01\n",
            "  -1.44122336e-01  9.22274984e-02 -1.05675016e+00 -7.86851642e-01\n",
            "   3.69026333e-01 -1.25246005e+00 -1.76112060e+00 -5.99336645e-01\n",
            "  -1.09886829e+00  4.54813897e-03  3.41763714e+00 -4.19820123e-02\n",
            "  -1.02108808e+00  2.06475154e+00 -4.83386178e-01 -2.39712136e-01\n",
            "   2.86152402e-01  6.01637766e-01  3.40634371e-01 -2.86014168e-01\n",
            "   1.48779870e+00 -9.93349357e-01  1.76222677e+00 -1.23413348e+00\n",
            "   1.55386449e-01 -1.06976509e+00  1.87249603e-01 -4.38364057e-01]]\n"
          ]
        }
      ]
    }
  ]
}